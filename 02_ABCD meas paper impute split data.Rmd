---
title: "ABCD measurement paper: Create training and test data"
author: "Meriah DeJoseph (edited by R. Sifre)"
date: "2020 - 2021"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r message=FALSE}
library(tidyverse)
# Multi-level modeling 
library(lme4)
library(lmerTest)
library(bbmle)
#library(AICcmodavg)

# Imputation 
library(mice)
library(miceadds)
library(finalfit)

# Train/test split
library(caret)

library(knitr)

knitr::opts_chunk$set(echo = TRUE)
```


# About
This file contains the code for making the training and held-out data. It assumes that there is a .csv called `clean_df.csv` in your working directory (generated by "ABCD meas paper prelim analyses" ).  
After reading in `clean_df.csv`, it imputes the missing independent variables, and saves it in your working directory as `imputed_dat.csv`. Then, it centers variables and creates variables that will be used for plotting. Finally, it splits the data into a training set (2/3) and a held-out set (1/3) and saves those .csvs in your working directory as `train_dat.csv` and `test_dat.csv`.


# Read data
```{r}
# Set working directory
setwd("~/Box/!ICD/ABCD/ANALYSES/!REVISION") #meriah's
#setwd("~/Box/ANALYSES") #robin's

df = read.csv(paste(getwd(), 'clean_df.csv', sep='/'))
```

# 1. Impute missing IVs

- Less than 10% of cases are missing on all IVs  
- Sanity check that there are no missing DVs in the cleaned data. 
```{r, warning=FALSE, echo=TRUE, include=TRUE}
my_matrix <- (df[, c("INR", "HIGHED", "DepETA", "ThreatETA", "SocETA",
                    "conWith", "dmnWith", "danWith", "fparWith", "salWith",
                    "vanWith","salDmn", "danVan", "conAmy_2h", "conHipp_2h",
                    "conAcc_2h")])

missing.prop <- colMeans(is.na(my_matrix))
round(missing.prop,3)
```

## Impute IVs  
Given low rates of missing data, we did not see variability in model estimates with different imputed datasets. For ease, we only use one imputed dataset moving forward.
```{r, warning=FALSE, message=FALSE, echo=TRUE, include=TRUE, cache=TRUE}
n_mice = 1

# Lists of variables that are explanator, dependent/outcome, and those that should not contribute to imputation (ID variablaes)
explanatory = c('agemo','FEMALE', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE', 'rsfmri_c_ngd_meanmotion',
             'HIGHED', 'INR','DepETA', 'ThreatETA', 'SocETA')

dependent = c('conWith', 'dmnWith', 'danWith', 'fparWith', 'salWith', 'vanWith','salDmn', 'danVan','conAmy_2h', 'conHipp_2h', 'conAcc_2h')

dropvars = c('SUBID','sitenum', 'rel_family_id') #drop vars not needed for imputation

# Create predictorMatrix for imputation (specifies which predictors to be used for imputing missing variables )
predM <- df %>% 
    select(dependent, explanatory, dropvars) %>% 
    finalfit::missing_predictorMatrix(
      # Do not impute missing cases for outcome variables  
        drop_from_imputed = c(dependent, dropvars), 
        # Do not use ID variables as info for imputing other variables bc they are meaningless
        drop_from_imputer = dropvars 
        ) 

# Create data frame with logicals of the same dimensions as data (specifies which variables should and should not be imputed) i
where.df = is.na(df %>% select(dependent, explanatory,dropvars))
where.df[, dependent] = 0 # Specify to not impute the outcome variables 
where.df = as.data.frame(where.df)

fits = df %>% select(dependent, explanatory, dropvars) %>% 
  mice(m = n_mice, seed = 5,
       predictorMatrix = predM, 
       where=where.df,
       drop_from_imputer = dropvars) 
```

## Imputation formulas
```{r}
fits$formulas
```


## Some checks and data prep on newly imputed data
```{r}
data_1 = complete(fits,1) 

# Some quick checks
sum(is.na(data_1$conWith)) 
sum(is.na(data_1$HIGHEDC)) 
sum(is.na(data_1$conWith)) 
sum(is.na(data_1$SocETA)) 
```



```{r, warning=FALSE, include=TRUE, cache=TRUE}
# Make mean-centered variables for modeling 
# # # # # # # # # # # # # # # # # # # # # # 
# Age
data_1$AGEC = (data_1$agemo- mean(data_1$agemo))
data_1$INRC = (data_1$INR- mean(data_1$INR))
data_1$HIGHEDC = (data_1$HIGHED- mean(data_1$HIGHED))

# Make quadratic variables for testing functional forms
# # # # # # # # # # # # # # # # # # # # # # # # # # # # 
data_1$INRC_2 = data_1$INRC^2
data_1$HIGHEDC_2 = data_1$HIGHEDC^2
data_1$DepETA_2 = data_1$DepETA^2
data_1$ThreatETA_2 = data_1$ThreatETA^2
data_1$SocETA_2 = data_1$SocETA^2

# Get median splits for moderators to plot 
# # # # # # # # # # # # # # # # # # # # # 
data_1 <- data_1 %>% 
  mutate(
    DepETAmed = ifelse(DepETA>=median(DepETA,na.rm=T), 1, 0),
    ThreatETAmed = ifelse(ThreatETA>=median(ThreatETA,na.rm=T), 1, 0),
    SocETAmed = ifelse(SocETA>=median(SocETA,na.rm=T), 1, 0)
  ) 

# Make factors 
# # # # # # # #
data_1$DepETAmed = as.factor(data_1$DepETAmed)
data_1$ThreatETAmed = as.factor(data_1$ThreatETAmed)
data_1$SocETAmed = as.factor(data_1$SocETAmed)
```

## Save imputed data
```{r}
write.csv(data_1, paste(getwd(), 'imputed_dat.csv', sep = '/'))
```


# 2. Make Train/test split 

 Partition data the same for all outcomes
```{r, warning=FALSE, include=TRUE, cache=TRUE}
set.seed(5) #set seed to ensure same answers every time
train.index <- createDataPartition(data_1$sitenum, p = .66, list = FALSE)
train <- data_1[ train.index,] #training set
test  <- data_1[-train.index,] #testing set
```
From here model fitting for each outcome will be done using the training data (`train`). Once final model is determined, we will test it on our test data to determine robustness of the model and its effects. 

```{r echo = TRUE}
write.csv(file = paste(getwd(), 'train_dat.csv', sep='/'), x= train)
write.csv(file = paste(getwd(), 'test_dat.csv', sep='/'), x= test)
```
